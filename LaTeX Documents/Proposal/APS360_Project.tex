\documentclass{article} % For LaTeX2e
\usepackage{iclr2022_conference,times}
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

%######## APS360: Uncomment your submission name
\newcommand{\apsname}{Project Proposal}
%\newcommand{\apsname}{Progress Report}
%\newcommand{\apsname}{Final Report}

%######## APS360: Put your Group Number here
\newcommand{\gpnumber}{6}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}

%######## APS360: Put your project Title here
\title{APS360 Project Proposal}


%######## APS360: Put your names, student IDs and Emails here
\author{Junwon Kang  \\
Student\# 1007390958\\
\texttt{junwon.kang@mail.utoronto.ca} \\
\And
Author Two  \\
Student\# 1005678901 \\
\texttt{author2@mail.utoronto.ca} \\
\AND
John Angelou  \\
Student\# 1006722520 \\
\texttt{john.angelou@mail.utoronto.ca} \\
\And
Author Four \\
Student\# 1005678901 \\
\texttt{author4@mail.utoronto.ca} \\
\AND
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy 
%######## APS360: Document starts here
\begin{document}

\maketitle

\begin{abstract}
This template should be used for all your project related reports in APS360 course. -- Write an abstract for your project here. Please review the \textbf{ First Course Tutorial} for a quick start
%######## APS360: Do not change the next line. This shows your Main body page count.
----Total Pages: \pageref{last_page}
\end{abstract}



\section{Introduction}

Pokemon originated as a video game series in Japan in 1996, later coming to the Untied States in 1998 [1]. Since its original release, pokemon is now one of the most popular media franchises of all time and is the 3rd best selling video game franchise with over 440 million copies sold worldwide [2]. In the games, there are fictional creatures called "pokemon" that come in many shapes and sizes. There are exactly 1008 pokemon in total spread between 9 different generations [3]. For this project, we scope down and focus on the first two generations of pokemon which is 251 pokemon in total. The goal of this project is to correctly classify images of pokemon within the first two generations using machine learning as a tool, specifically deep learning. Deep learning is a great approach for this project as this is mainly an image classification project. Pokemons have many different distinct features that put them apart, meaning there are many features to extract and use. Finally, using deep learning to classify images of pokemon has interesting implications for the future as it may advance pokemon games to the point where the player can get information about each pokemon by using mods to identify the pokemon in real time.

\section{Illustration}

\graphicspath{ {./imgs/} }
\includegraphics[width=\textwidth]{imgs/Modelillustration.png}
\caption {\centerline{Figure 1. Overall idea and architecture of the model}}

\section{Background and Related Work}

Previous work on using machine learning as a tool for image classification of pokemon has been done by previous students of different universities. The first related work [4] focuses on using pokemon trading cards as the input for their model to output a customized trading card that follows a consistent design with pokemon trading cards. Their work consists of image generation, text generation, image classification, and text classification. 
This paper however will focus strictly on image classification. The authors of the first related work use a convolutional neural network to classify their generated pokemon image. The second related work [5] focuses on classifying pokemon based on a limited amount of training data, specifically only using one image for the training set and one image for the test set. This paper will also focus on classifying pokemon, but with using more data to train our network. Their work consists of using two different instances of embedding networks to separate images with blank backgrounds and diverse backgrounds. It also uses saliency detection to obtain background objects to mask them. 

\section{Data Processing}

In terms of the data needed for this model, an equal amount of clear and recognizable images for each pokemon from the first two generations is required. After this, each image would then need to be rescaled to one size, ensuring that both our CNN and baseline model can train and classify each image. The first step to this process was to obtain all the images needed, to which it was decided that each pokemon should each have 20 images. This process was done by hand, and was collected primarily from three sources. The first, and most important source used was the Bulbagarden Wikipedia Archives [Bulb], which contains images in various different styles (such as hand drawn, pixel art, updated art from newer games, etc) for all pokemon needed.Next, images were also taken from pre-existing datasets, with the most influential dataset being a data archive of 7000 labeled pokemon created by Lance Zhang [Lance]. Finally, images were also scraped from google images, with an emphasis on obtaining unique pokemon images that did not appear within the other two datasets. Once this data is collected, the images are then manually cropped using making sure that the pokemon is still recognizable. Finally, data augmentation will be applied to each image, further increasing the amount of data available. 

\section {Architecture}

As our goal in this project is to train a neural network to both recognize and classify images of Pokemon fed to it, we first make efforts to understand current research surrounding image recognition and classification models in order to uncover the best architecture. Although a popular and arguably the simplest choice would be using the multi-layer perceptrons (MLP), such a choice also holds many drawbacks, making it a suboptimal choice overall. Many of these drawbacks include inability to adapt to different data sizes, its poor inductive bias, and the fact that each new connected layer must be hand-coded into the model, creating tedious amounts of work to improve hyperparameter optimization. By contrast, CNN's provide us with great amounts of flexibility in terms of our inputs, while not sacrificing the quality of predictions. CNN's learn local features of an image by spanning features of an input into lower dimensions, and detecting these characteristics locally with the use of filters. In other words, through a process called convulsion, the model first tries to learn general concepts (lines, shapes), in lower dimensions, and then uses these learned features to identify images. 

CNN's themselves come in a large variety of forms, each with their own advantages and disadvantages. Given the stage of the project at which we are at right now, we cannot be certain as to which architecture would be the best for the Pokemon classification. Resultantly, we have decided to explore three different CNN architectures, and to evaluate how our model performs against each one of these. In the future, we can continue with progress with one or many of these architectures. The architectures which we are aiming to implement on would be the simple alexNet architecture, the improved VGG-19 architecture, and finally the very modern ResNet-34 architecture. 

The alexNet architecture, perhaps one of the more basic implementations of a CNN, is the first architecture which we are looking to implement. It is composed of 8 layers - 5 convolution and 3 fully connected. Using 16 million neurons, it maintains its authenticity of feature recognition dropout, ReLu activation functions, and intermittent MaxPooling through the network. While it may be argued that such a provably worse architecture would be a purposeless implementation, we present a few arguments for its implementation. Firstly, the use of such a simple implementation for our model can easily point out major flaws within our data processing task, and can also help in initializing our hyperparameter optimization. Additionally, the AlexNet implementation, while simple, does not tend to give poor results, and oftentimes performs comparably to other CNN architectures.

The VGG-19 (Visual Geometry Group 19) architecture is a direct improvement of the AlexNet architecture involving utilizing 19 layers of convolution, with an increasing amount of 3x3 filters used as it goes deeper into the network. In VGG, every layer is connected to the previous layer, such that it only receives its inputs from the previous layer. The VGG network is composed of 16 feed-forward convolutional layers, along with three fully connected layers at the end. Again, MaxPooling is used intermittently at certain convolutions to ensure that the most salient features are included within the feature recognition section. 

Finally, our last architecture to be included would be the ResNet-34 architecture. The ResNet-34 is considered to be one of the most modern iterations of the AlexNet architecture, and is also considered to be the most accurate of the aforementioned CNNs. Boasting 34 layers, it contains a mixture of both MaxPooling and AveragePooling (although these can be replaced with a stride of 2), along with 33 convolutional layers and 1 fully connected layer. The model utilizes skip connections to provide deeper layers with a stronger signal, which mitigates the vanishing gradient problem which the batch normalization of the previous models could not solve. By most research, this model is expected to perform the best compared to the previous implementations; we hope to see this in our implementation of Pokemon image generation as well. 

An important property of CNNs that we will also aim to take advantage of transfer learning within our model. This property involves taking pre-trained variations of the three architectures above. The pre-training involves training the network to recognize shapes, images, lines, and general patterns in a dataset of over 15 million images, effectively allowing us to skip over the convolutional feature training section of our above models. Not only will this save us a lot of time, it could potentially provide better results given these networks have been trained on a much vaster dataset than any amount of data we can procure from Pokemon alone. Nonetheless, we should be cognizant of the fact that this method may falter as well, given the nature of Pokemon as hand drawn images rather than real life images. Other hyperparameters, such as batch-size, epoch number, learning rate, and more have also been considered, and we will try to choose the most optimal hyperparameters through experimentation and allowing the model to learn the best options. For our loss function, we have chosen Cross Entropy (above models include softmax) with one-hot Encoding of ground truth labels given its prominent use in classification problems such as this.

\section {Baseline Model}

In order to support our chosen CNN model, a Support Vector Machine (SVM) classification model was chosen as a baseline. This is a simple machine learning model that works by plotting each data point within a dimensional space, finding a decision boundary that separates each data point by class [J1], effectively classifying each image. In order to implement this, data is first flatted from an RGB three dimensional array to a 2D array [J2], with python library skikit-learn then training and fitting the data to an SVM model. This model was chosen due to its relatively high accuracy within certain image classification tests [J3][J4]; in certain cases even performing better than CNN models with smaller datasets [J4], indicating that having an SVM baseline model will help strengthen the performance of both our CNN model and dataset if implemented.

\section {Ethical Consideration}

\begin{itemize}
  \item Ethical issues in data collection (scraping images)
  \begin{itemize}
    \item Copyright issues: When we use python to scrape images from google or other relative websites to obtain data for our training\&testing datasets, we might unintentionally download images that copyrights may protect. Or, there can be images that its creators have forbidden to use in AI training.
    \item Sensitive images: When we are randomly downloading images from google or other websites, we can unintentionally download images that are sensitive, and that might be appropriate to all people. For example, there can be images containing sexual, political, or violent content, which will not be suitable for all people.
    \item For images that we scraped, there can be images containing private personal information, such as an image’s creator’s personal information, identity, location, time, etc. 
    \item Possible solutions: To maximally avoid these problems, we can sensor all images that we are using as our dataset, and remove all inappropriate images, thus maximally preventing ethical issues in data collecting.
  \end{itemize}
  \item Ethical issues in using the model (predicting a pokemon’s name/type using a image)
  \begin{itemize}
    \item Using the model maliciously: Use human face images as input data, and uses the result generated to attack/insult others
    \item Accuracy: The model cannot predict the pokemon 100\% correctly. If we are using this model to work on very formal work, the result can be very biased.
  \end{itemize}
\end{itemize}

\section {Project Plan}
Each team member is responsible for working with the team and ensuring that they are communicating with the team on a timely and efficient basis. They are responsible for ensuring that they are notifying the team of the tasks which they are completing currently, as well as tasks which they may need help on. They are also required to attend all set meetings and provide input at these meetings as well. Our team will encourage an environment of openness and communication; if a team member has an issue with a certain aspect of the project they are encouraged to speak up about it. Our team holistically will work together by meeting regularly not only for project discussion/planning, but also for coding sessions and peer-review coding time. This will ensure that all team members are involved in the process of building the model, and can mitigate potential errors/concerns. We will also have a set meeting time on Sunday at 4:00 PM every week to discuss the general direction of the project and week-by-week goals. We will meet for coding sessions on an ad-hoc basis, however through using When2Meet we can devise a time which works optimally for all of us (however, these meetings may not necessarily need to be attended by everyone all the time).  Overall, we will work with a Waterfall style of software development, where we will work together to finish the planning and analysis of the project and its requirements, before moving into the design and implementation of the project. This is done to ensure that the entire team understands the motivation and decisions of the project, given its complexity. 
We will mainly communicate with each other through two platforms: Instagram direct messages and Discord. Instagram will be used to communicate small aspects of the project, notifying team members when they are working on code, and asking simple questions which can be answered without the need of a meeting. It can be used to give general status updates to the project, along with facilitating asynchronous discussion surrounding the project, however longer-term changes should be discussed synchronously and within the meeting slot which works best for all team members. Discord can be used to send larger blocks of code, website URL’s, PDF’s, and longer messages to team members. We can keep track of past sent links in Discord easier, and can have much longer conversations easier through the platform as well. Our asynchronous meeting will also be hosted on Discord. We have shared both a Google Drive and a Github repository with the team. The google drive contains folders with spreadsheets for data, evidence of data collection (the URL’s where data is selected from can be sent through Discord), and drafts for the project plan. 

To ensure that we do not overwrite each other's code, we will work in a methodology which is inspired by how certain software companies operate. While we are not working with real time clients, we will have access to all the tasks which will be laid out and decided on by the team. Every week, the task list will be updated, such that there are new tasks reflecting the state of the system at the time. Team members can put down their names on tasks which they would like to do for that week, and will notify the team in the Instagram DM chat about the tasks they are taking that week. Each team member is expected to take at least two tasks, but may take more if needed. Total task amounts will be tallied, and people who are falling behind in tasks are expected to take on more to ensure equality. While this system may seem complex, we will ensure clarity by providing an open document with all the tasks clearly labeled and described for team members to understand. This also ensures a very liberal environment for team members to choose tasks which they are comfortable with, rather than being assigned tasks arbitrarily. Of course, multiple team members can also work on the same task as well, especially for more complex tasks.

\begin{table}[t]
\caption{Sample table title}
\label{sample-table}
\begin{center}
\begin{tabular}{ll}
\multicolumn{1}{c}{\bf PART}  &\multicolumn{1}{c}{\bf DESCRIPTION}
\\ \hline \\
Dendrite         &Input terminal \\
Axon             &Output terminal \\
Soma             &Cell body (contains cell nucleus) \\
\end{tabular}
\end{center}
\end{table}

\section {Risk Register}

Within this project, there are risks that may detrimentally impact the project’s timeline, 
completion, and performance. 

The first risk involves the dataset itself. There may arise a situation where the images used for the project may contain identical images due to a variety of factors such as accidentally retrieving identical photos from different sources. With this in mind, it may be possible for images used in training to be leaked to the test dataset. The likelihood of this risk is moderate as we are retrieving the images of generation 2 pokemon ourselves. If this risk were to occur, the team would cut down the dataset to contain distinct pokemon images and reorient them to make up for the lost data.

The second risk is that a team member may decide to drop the course sometime in the future. This would greatly affect the project plan as it is currently divided up equally amongst the four group members. The likelihood of this risk occurring is moderate as students tend to drop courses later on after midterm grades are out. If this risk were to occur, the group would reevaluate the project plan and divide up the work and readjust the project timeline accordingly. 

The third risk is that classes may not be balanced after splitting up the data into train, validation, and test sets. For example, the distribution of samples for different pokemon or classes may be skewed towards the train dataset which can leave some classes undersampled. The likelihood of this risk occurring is moderate if we are not careful in how we split the data. To prevent this risk from occurring, the group will ensure that all classes within our dataset will be distributed equally amongst the train, validation, and test sets. If this risk were to occur, then the team will incorporate a function within our code to manually split the classes evenly amongst the three sets, causing the data to be balanced. 

\subsection{Style}

Papers to be submitted to APS360 must be prepared according to the
instructions presented here.

Authors are required to use the APS360 \LaTeX{} style files obtainable at the
APS360 website on Quercus. Tweaking the style is not permitted.

\subsection{Retrieval of style files}

The file \verb+APS360_Project.pdf+ contains these
instructions and illustrates the various formatting requirements your APS360 paper must satisfy.
Submissions must be made using \LaTeX{} and the style files
\verb+iclr2022_conference.sty+ and \verb+iclr2022_conference.bst+ (to be used with \LaTeX{}2e). The file
\verb+APS360_Project.tex+ may be used as a ``shell'' for writing your paper. All you have to do is replace the author, title, abstract, and text of the paper with
your own.

The formatting instructions contained in these style files are summarized in
sections \ref{gen_inst}, \ref{headings}, and \ref{others} below.

\section{General formatting instructions}
\label{gen_inst}

The text must be confined within a rectangle 5.5~inches (33~picas) wide and
9~inches (54~picas) long. The left margin is 1.5~inch (9~picas).
Use 10~point type with a vertical spacing of 11~points. Times New Roman is the
preferred typeface throughout. Paragraphs are separated by 1/2~line space,
with no indentation.

Paper title is 17~point, in small caps and left-aligned.
All pages should start at 1~inch (6~picas) from the top of the page.

Authors' names are
set in boldface, and each name is placed above its corresponding
address. The lead author's name is to be listed first, and
the co-authors' names are set to follow. Authors sharing the
same address can be on the same line.

Please pay special attention to the instructions in section \ref{others}
regarding figures, tables, acknowledgments, and references.


There will be a strict upper limit of 9 pages for the main text of the initial submission, with unlimited additional pages for citations. 

\section{Headings: first level}
\label{headings}

First level headings are in small caps,
flush left and in point size 12. One line space before the first level
heading and 1/2~line space after the first level heading.

\subsection{Headings: second level}

Second level headings are in small caps,
flush left and in point size 10. One line space before the second level
heading and 1/2~line space after the second level heading.

\subsubsection{Headings: third level}

Third level headings are in small caps,
flush left and in point size 10. One line space before the third level
heading and 1/2~line space after the third level heading.

\section{Citations, figures, tables, references}
\label{others}

These instructions apply to everyone, regardless of the formatter being used.

\subsection{Citations within the text}

Citations within the text should be based on the \texttt{natbib} package
and include the authors' last names and year (with the ``et~al.'' construct
for more than two authors). When the authors or the publication are
included in the sentence, the citation should not be in parenthesis using \verb|\citet{}| (as
in ``See \citet{Hinton06} for more information.''). Otherwise, the citation
should be in parenthesis using \verb|\citep{}| (as in ``Deep learning shows promise to make progress
towards AI~\citep{Bengio+chapter2007}.'').

The corresponding references are to be listed in alphabetical order of
authors, in the \textsc{References} section. As to the format of the
references themselves, any style is acceptable as long as it is used
consistently.

To cite a new paper, first, you need to add that paper's BibTeX information to \verb+APS360_ref.bib+ file and then you can use the \verb|\citep{}| command to cite that in your main document. 

\subsection{Footnotes}

Indicate footnotes with a number\footnote{Sample of the first footnote} in the
text. Place the footnotes at the bottom of the page on which they appear.
Precede the footnote with a horizontal rule of 2~inches
(12~picas).\footnote{Sample of the second footnote}

\subsection{Figures}

All artwork must be neat, clean, and legible. Lines should be dark
enough for purposes of reproduction; art work should not be
hand-drawn. The figure number and caption always appear after the
figure. Place one line space before the figure caption, and one line
space after the figure. The figure caption is lower case (except for
first word and proper nouns); figures are numbered consecutively.

Make sure the figure caption does not get separated from the figure.
Leave sufficient space to avoid splitting the figure and figure caption.

You may use color figures.
However, it is best for the
figure captions and the paper body to make sense if the paper is printed
either in black/white or in color.

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.6\textwidth]{Figs/td-deep-learning.jpg}
\end{center}
\caption{Sample figure caption. Image: ZDNet}
\end{figure}

\subsection{Tables}

All tables must be centered, neat, clean and legible. Do not use hand-drawn
tables. The table number and title always appear before the table. See
Table~\ref{sample-table}.

Place one line space before the table title, one line space after the table
title, and one line space after the table. The table title must be lower case
(except for first word and proper nouns); tables are numbered consecutively.

\begin{table}[t]
\caption{Sample table title}
\label{sample-table}
\begin{center}
\begin{tabular}{ll}
\multicolumn{1}{c}{\bf PART}  &\multicolumn{1}{c}{\bf DESCRIPTION}
\\ \hline \\
Dendrite         &Input terminal \\
Axon             &Output terminal \\
Soma             &Cell body (contains cell nucleus) \\
\end{tabular}
\end{center}
\end{table}

\section{Default Notation}

In an attempt to encourage standardized notation, we have included the
notation file from the textbook, \textit{Deep Learning}
\cite{goodfellow2016deep} available at
\url{https://github.com/goodfeli/dlbook_notation/}.  Use of this style
is not required and can be disabled by commenting out
\texttt{math\_commands.tex}.


\centerline{\bf Numbers and Arrays}
\bgroup
\def\arraystretch{1.5}
\begin{tabular}{p{1in}p{3.25in}}
$\displaystyle a$ & A scalar (integer or real)\\
$\displaystyle \va$ & A vector\\
$\displaystyle \mA$ & A matrix\\
$\displaystyle \tA$ & A tensor\\
$\displaystyle \mI_n$ & Identity matrix with $n$ rows and $n$ columns\\
$\displaystyle \mI$ & Identity matrix with dimensionality implied by context\\
$\displaystyle \ve^{(i)}$ & Standard basis vector $[0,\dots,0,1,0,\dots,0]$ with a 1 at position $i$\\
$\displaystyle \text{diag}(\va)$ & A square, diagonal matrix with diagonal entries given by $\va$\\
$\displaystyle \ra$ & A scalar random variable\\
$\displaystyle \rva$ & A vector-valued random variable\\
$\displaystyle \rmA$ & A matrix-valued random variable\\
\end{tabular}
\egroup
\vspace{0.25cm}
\centerline{\bf Sets and Graphs}
\bgroup
\def\arraystretch{1.5}

\begin{tabular}{p{1.25in}p{3.25in}}
$\displaystyle \sA$ & A set\\
$\displaystyle \R$ & The set of real numbers \\
$\displaystyle \{0, 1\}$ & The set containing 0 and 1 \\
$\displaystyle \{0, 1, \dots, n \}$ & The set of all integers between $0$ and $n$\\
$\displaystyle [a, b]$ & The real interval including $a$ and $b$\\
$\displaystyle (a, b]$ & The real interval excluding $a$ but including $b$\\
$\displaystyle \sA \backslash \sB$ & Set subtraction, i.e., the set containing the elements of $\sA$ that are not in $\sB$\\
$\displaystyle \gG$ & A graph\\
$\displaystyle \parents_\gG(\ervx_i)$ & The parents of $\ervx_i$ in $\gG$
\end{tabular}
\vspace{0.25cm}


\centerline{\bf Indexing}
\bgroup
\def\arraystretch{1.5}

\begin{tabular}{p{1.25in}p{3.25in}}
$\displaystyle \eva_i$ & Element $i$ of vector $\va$, with indexing starting at 1 \\
$\displaystyle \eva_{-i}$ & All elements of vector $\va$ except for element $i$ \\
$\displaystyle \emA_{i,j}$ & Element $i, j$ of matrix $\mA$ \\
$\displaystyle \mA_{i, :}$ & Row $i$ of matrix $\mA$ \\
$\displaystyle \mA_{:, i}$ & Column $i$ of matrix $\mA$ \\
$\displaystyle \etA_{i, j, k}$ & Element $(i, j, k)$ of a 3-D tensor $\tA$\\
$\displaystyle \tA_{:, :, i}$ & 2-D slice of a 3-D tensor\\
$\displaystyle \erva_i$ & Element $i$ of the random vector $\rva$ \\
\end{tabular}
\egroup
\vspace{0.25cm}


\centerline{\bf Calculus}
\bgroup
\def\arraystretch{1.5}
\begin{tabular}{p{1.25in}p{3.25in}}
% NOTE: the [2ex] on the next line adds extra height to that row of the table.
% Without that command, the fraction on the first line is too tall and collides
% with the fraction on the second line.
$\displaystyle\frac{d y} {d x}$ & Derivative of $y$ with respect to $x$\\ [2ex]
$\displaystyle \frac{\partial y} {\partial x} $ & Partial derivative of $y$ with respect to $x$ \\
$\displaystyle \nabla_\vx y $ & Gradient of $y$ with respect to $\vx$ \\
$\displaystyle \nabla_\mX y $ & Matrix derivatives of $y$ with respect to $\mX$ \\
$\displaystyle \nabla_\tX y $ & Tensor containing derivatives of $y$ with respect to $\tX$ \\
$\displaystyle \frac{\partial f}{\partial \vx} $ & Jacobian matrix $\mJ \in \R^{m\times n}$ of $f: \R^n \rightarrow \R^m$\\
$\displaystyle \nabla_\vx^2 f(\vx)\text{ or }\mH( f)(\vx)$ & The Hessian matrix of $f$ at input point $\vx$\\
$\displaystyle \int f(\vx) d\vx $ & Definite integral over the entire domain of $\vx$ \\
$\displaystyle \int_\sS f(\vx) d\vx$ & Definite integral with respect to $\vx$ over the set $\sS$ \\
\end{tabular}
\egroup
\vspace{0.25cm}

\centerline{\bf Probability and Information Theory}
\bgroup
\def\arraystretch{1.5}
\begin{tabular}{p{1.25in}p{3.25in}}
$\displaystyle P(\ra)$ & A probability distribution over a discrete variable\\
$\displaystyle p(\ra)$ & A probability distribution over a continuous variable, or over
a variable whose type has not been specified\\
$\displaystyle \ra \sim P$ & Random variable $\ra$ has distribution $P$\\% so thing on left of \sim should always be a random variable, with name beginning with \r
$\displaystyle  \E_{\rx\sim P} [ f(x) ]\text{ or } \E f(x)$ & Expectation of $f(x)$ with respect to $P(\rx)$ \\
$\displaystyle \Var(f(x)) $ &  Variance of $f(x)$ under $P(\rx)$ \\
$\displaystyle \Cov(f(x),g(x)) $ & Covariance of $f(x)$ and $g(x)$ under $P(\rx)$\\
$\displaystyle H(\rx) $ & Shannon entropy of the random variable $\rx$\\
$\displaystyle \KL ( P \Vert Q ) $ & Kullback-Leibler divergence of P and Q \\
$\displaystyle \mathcal{N} ( \vx ; \vmu , \mSigma)$ & Gaussian distribution %
over $\vx$ with mean $\vmu$ and covariance $\mSigma$ \\
\end{tabular}
\egroup
\vspace{0.25cm}

\centerline{\bf Functions}
\bgroup
\def\arraystretch{1.5}
\begin{tabular}{p{1.25in}p{3.25in}}
$\displaystyle f: \sA \rightarrow \sB$ & The function $f$ with domain $\sA$ and range $\sB$\\
$\displaystyle f \circ g $ & Composition of the functions $f$ and $g$ \\
  $\displaystyle f(\vx ; \vtheta) $ & A function of $\vx$ parametrized by $\vtheta$.
  (Sometimes we write $f(\vx)$ and omit the argument $\vtheta$ to lighten notation) \\
$\displaystyle \log x$ & Natural logarithm of $x$ \\
$\displaystyle \sigma(x)$ & Logistic sigmoid, $\displaystyle \frac{1} {1 + \exp(-x)}$ \\
$\displaystyle \zeta(x)$ & Softplus, $\log(1 + \exp(x))$ \\
$\displaystyle || \vx ||_p $ & $\normlp$ norm of $\vx$ \\
$\displaystyle || \vx || $ & $\normltwo$ norm of $\vx$ \\
$\displaystyle x^+$ & Positive part of $x$, i.e., $\max(0,x)$\\
$\displaystyle \1_\mathrm{condition}$ & is 1 if the condition is true, 0 otherwise\\
\end{tabular}
\egroup
\vspace{0.25cm}



\section{Final instructions}
Do not change any aspects of the formatting parameters in the style files.
In particular, do not modify the width or length of the rectangle the text
should fit into, and do not change font sizes (except perhaps in the
\textsc{References} section; see below). Please note that pages should be
numbered.


\subsubsection*{Author Contributions}
If you'd like to, you may include  a section for author contributions as is done
in many journals. This is optional and at the discretion of the authors.

\subsubsection*{Acknowledgments}
Use unnumbered third level headings for the acknowledgments. All
acknowledgments, including those to funding agencies, go at the end of the paper.

\label{last_page}

\bibliography{APS360_ref}
\bibliographystyle{iclr2022_conference}

\end{document}
